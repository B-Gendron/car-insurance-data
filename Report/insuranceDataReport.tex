\documentclass[a4paper,11pt, titlepage]{article}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{epsf}
\usepackage[left=2.5cm,right=2.5cm,top=2cm,bottom=2cm]{geometry}
\usepackage{float}
\usepackage{lmodern} 
\usepackage[table]{xcolor}
\usepackage{multirow}
\usepackage{listings} 
\usepackage[justification=centering]{caption}
\usepackage{hyperref}
\usepackage[cc]{titlepic}
\usepackage{fancyhdr}
\usepackage{etoolbox}
\usepackage{wrapfig}

%ttfamily

\lstset{% 
language=r, 
basicstyle=\scriptsize, 
keywordstyle=\color{blue}\bfseries, 
commentstyle=\color{black!20!green}, 
stringstyle=\color{red}, 
numbers=left,
numberstyle=\tiny,
frame=\lines
} 

\title{\Huge{Machine learning models for claim prediction in car insurance}}
\author{\Large{Barbara {\sc Gendron-Audebert}}}
\date\today
\titlepic{\includegraphics[width=\textwidth]{cover-image.png}}

\patchcmd{\maketitle}
  {\end{titlepage}}
  {\thispagestyle{titlepagestyle}\end{titlepage}}
  {}{}

\fancypagestyle{titlepagestyle}
{
   \fancyhf{}
   \fancyfoot[C]{\emph{A car facing dangers in a hostile planet, protected with an umbrella}, DALL.E generated illustration}
   \renewcommand{\headrulewidth}{0 mm}
}


% --------------------------------------------------------------

\begin{document}
\maketitle
\tableofcontents
\vskip150pt
\section*{Foreword}

This report gives some insights about the use of machine learning techniques in car insurance context. The goal of this work is to leverage data on policyholders related to their driving experience to predict the occurence of a claim. The dataset used to conduct this analysis comes from \href{https://www.kaggle.com/datasets/sagnik1511/car-insurance-data}{this Kaggle page}.\\
\\
In the following sections, we first explore the dataset and describe the related insurance problem in part \ref{data}, before diving in a deeper analysis of the provided features in part \ref{analysis}. In part \ref{models}, one can find insights about the models used to solve this problem, such as a representation of the decision process leading to the model predictions. Finally, part \ref{results} brings a discussion about the provided results, along with some take-aways of this study about car insurance claim prediction.\\
\\
All the technical specifications, the data, all the plots and especially the code (in R) are available in the attached files of the report, and online in \href{https://github.com/B-Gendron/car-insurance-data}{this Git repository}.

\newpage

\section{Data and related insurance problem} \label{data}

\subsection{Dataset description}

The above mentioned dataset contains 19 pieces of information (for now denoted as \textsl{features}) for 10000 policyholders. Among such features, some are closely related to the driving behaviour of the policyholder (driving experience, number of past accidents and speeding violations, \dots), whereas other are more related to its living conditions and family (age, education, income, \dots). Lastly, there is a feature to indicate whether or not the policyholder already experienced a claim, denoted as {\tt OUTCOME}. For a complete description of the features, please refer to table \ref{data-description} in the appendix. \\
\\
\noindent Most of the features names are clear enough at first sight but some need to be clarified. The credit score reflect the ability for a policyholder to pay for his debts. The higher the score, the more creditworthy the policyholder is. In car insurance, it is has been observed that this parameter has a significant influence on the insurance rate. The feature {\tt DUIS} refers to DUI, which stands for \textsl{driving under the influence} (whether it be alcohol or drugs).

\subsection{The insurance context}

Machine learning models can be used in the car insurance context for claim prediction by analyzing historical claims data and identifying patterns and trends that can help predict the likelihood of future claims for given policyholders. This can allow insurance companies to better assess risk and price policies accordingly, potentially leading to cost savings for both the insurer and the insured. In this case, the aim is to predict the {\tt OUTCOME} feature from the others, using some machine learning models. 

% -------------------    ANALYSIS    -------------------- % 

\section{Preliminary analysis of the data} \label{analysis}

The purpose of this section is to give a more quantitative description of the data and to go over points of attention to ensure proper modeling.
\begin{itemize}
    \item Deal with outliers and missing values
    \item Check for the balance of the data with respect to the {\tt OUTCOME} feature
    \item Check for remaining anomalies in the data 
    \item Select only the relevant features
\end{itemize}

\subsection{Outliers and missing values}

First, it is usual to compute some basic statistics about each feature on the whole data, such as the minimum and the maximum values, the mean and median. This allows to notice rough anomalies, such as a negative age values. In this dataset, it appear that no anomalies of this type were found.\\
\\
\noindent Descriptive statistics used for this step can be displayed simply using the function {\tt summary()} in R. They are provided along with the number of NA's values for each feature, which corresponds to missing values (NA stands for \textsl{Not Available}). Here, the features {\tt CREDIT SCORE} and {\tt ANNUAL MILEAGE} respectively have 982 and 957 NA's values. This represents approximately 1\% of the whole data for each variable, that's why we can consider simply delete them. Thus, the remaining dataset contains 8149 rows. 

\subsection{Data balance with respect to the {\tt OUTCOME} feature}

\begin{wrapfigure}{r}{5.5cm}
    \centering
    \includegraphics[width=5cm]{outcome.png}
    \caption{\centering Distribution of the {\tt OUTCOME} feature.}
\end{wrapfigure}

Insurance claims are rare events, so there is typically not a lot of data available about them. This limited availability of data on insurance claims can lead to challenges when using machine learning models, as it is well-known that such models require a significant amount of data in order to perform well. Therefore, if the data is too imbalanced with respect to the {\tt OUTCOME} (far more "no" than "yes"), the model won't be able to learn well about the claim, which is precisely the point here.


\subsection{Remaining anomalies in the data}

\subsection{Select only the relevant features}

% -------------------    MODELS    -------------------- % 

\section{Brief description of the models used} \label{models}


% -------------------    RESULTS    -------------------- % 

\section{Analysis of the results and conclusion} \label{results}
\rowcolors{2}{white}{white}
\begin{table}[h!]
    \begin{tabular}[t]{|c|ccccc|}
        \rowcolor{orange!30}
\hline
\textbf{Model} & \textbf{Accuracy} & \textbf{Sensitivity} & \textbf{Specificity} & \textbf{PPV} & \textbf{NPV} \\
\hline
Logistic regression         & \underline{0.8245} & 0.8629 & \underline{0.7334} & \underline{0.8850} & 0.6923 \\
Decision Tree Classifier    & 0.8164          & 0.8534 & 0.7252 & 0.8844 & 0.6675 \\
Random Forest Classifier    & \underline{0.8245} & \underline{0.8719} & 0.7206 & 0.8725 & 0.7197 \\
XGBoost                     & 0.8164          & 0.8844 & 0.6675 & 0.8534 & \underline{0.7252} \\
\hline
    \end{tabular}
\centering
\caption{A sum-up table of the classification metrics for each model. PPV stands for Predicted Positive Values and NPV stands for Negative Predicted Values.}
\label{metrics}
\end{table}%

\appendix

% -------------------    APPENDIX    -------------------- % 

\section{Appendix}

\rowcolors{2}{gray!20}{white}

\begin{table}[ht]
    \begin{tabular}[t]{|lcc|}
        \rowcolor{orange!30}
\hline
\textbf{Variable}           & \textbf{Type}      & \textbf{Value ranges (if meaningful)}     \\
\hline
VEHICLE OWNERSHIP   & Binary    & 0 or 1                     \\
MARRIED             & Binary    & 0 or 1                     \\
CHILDREN            & Binary    & 0 or 1                    \\
OUTCOME             & Binary    & 0 or 1                     \\
AGE                 & Category    & 16-25, 26-39, 40-64, 65+      \\
GENDER              & Category    & female, male                  \\
RACE                & Category    & majority, minority            \\
DRIVING EXPERIENCE  & Category    & 0-9y, 10-19y, 20-29y, 30y+     \\
EDUCATION           & Category    & high school, none, university \\
INCOME              & Category    & middle class, poverty, upper class, working class \\
VEHICLE TYPE        & Category    & sedan, sports car             \\
VEHICLE YEAR        & Category    & after 2015, before 2015       \\
CREDIT SCORE        & Float     & From 0.0534 to 0.9608       \\
ID                  & Integer   & --                \\
POSTAL CODE         & Integer   & --             \\ 
ANNUAL MILEAGE      & Integer   & From 2000 to 22000               \\ 

SPEEDING VIOLATIONS & Integer   & From 0 to 22                   \\
DUIS                & Integer   & From 0 to 6                      \\ 
PAST ACCIDENTS      & Integer   & From 0 to 15                    \\  

\hline
    \end{tabular}
\centering
\caption{A short description of the covariates, along with some insights about categorical variables.}
\label{data-description}
\end{table}%

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.35]{eda.png}
    \caption{Histograms of the numerical variables.}
    \label{fig:histograms}
\end{figure}


\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.3]{eda-report.png}
    \caption{Value counts of categorical features with respect to the {\tt OUTCOME} value.}
    \label{fig:categorical-counts}
\end{figure}

\end{document}
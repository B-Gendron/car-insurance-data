library(caret)
library(corrplot)
library(cowplot)
library(data.table)
library(ggplot2)
library(gridExtra)
library(mlr)
library(mltools)
library(randomForest)
library(rpart)
library(rpart.plot)
library(tidyverse)
library(xgboost)
data <- read.table("Car_Insurance_Claim.csv", header=T, sep=',')
data = data%>%mutate_if(is.character, as.factor)
data$VEHICLE_OWNERSHIP = as.factor(data$VEHICLE_OWNERSHIP)
data$MARRIED = as.factor(data$MARRIED)
data$CHILDREN = as.factor(data$CHILDREN)
data$OUTCOME = as.factor(data$OUTCOME)
data <- na.omit(data)
summary(data)
data_processed <- data[,!names(data) %in% c('GENDER', 'POSTAL_CODE', 'EDUCATION', 'RACE', 'ID', 'VEHICLE_TYPE')]
summary(data_processed)
set.seed(42) # set random seed for reproducibility
# Train/test split
trainIndex = sample(1:nrow(data_processed), size = round(0.7*nrow(data_processed)), replace=FALSE)
train = data_processed[trainIndex ,]
test = data_processed[-trainIndex ,]
logreg <- glm(OUTCOME~., data=train, family=binomial)
summary(logreg)
proba_logreg <- predict(logreg, newdata=test, type="response")
pred_logreg <- ifelse(proba_logreg<0.5, 0, 1)
pred_logreg <- factor(pred_logreg)
MC_logreg <- confusionMatrix(as.factor(test$OUTCOME), pred_logreg)
MC_logreg
decision_tree <- rpart(OUTCOME~., data=train)
rpart.plot(decision_tree)
pred_tree <- predict(object=decision_tree, newdata=test, type="class")
MC_tree <- confusionMatrix(test$OUTCOME, pred_tree)
rf <- randomForest(as.factor(OUTCOME)~., data=train, proximity=TRUE)
pred_rf <- predict(rf, test)
MC_rf <- confusionMatrix(test$OUTCOME, pred_rf)
rf <- randomForest(as.factor(OUTCOME)~., data=train, proximity=TRUE)
pred_rf <- predict(rf, test)
MC_rf <- confusionMatrix(test$OUTCOME, pred_rf)
train$OUTCOME = as.numeric(levels(train$OUTCOME))[train$OUTCOME]
test$OUTCOME = as.numeric(levels(test$OUTCOME))[test$OUTCOME]
X_train <- model.matrix(~ INCOME + VEHICLE_YEAR +
SPEEDING_VIOLATIONS  + AGE + DRIVING_EXPERIENCE +
CREDIT_SCORE + MARRIED + ANNUAL_MILEAGE + DUIS +
VEHICLE_OWNERSHIP + CHILDREN +
PAST_ACCIDENTS,data=train)
X_test <- model.matrix(~ INCOME + VEHICLE_YEAR +
SPEEDING_VIOLATIONS  + AGE + DRIVING_EXPERIENCE +
CREDIT_SCORE + MARRIED + ANNUAL_MILEAGE + DUIS +
VEHICLE_OWNERSHIP + CHILDREN +
PAST_ACCIDENTS,data=test)
labels <- train$OUTCOME
ts_label <- test$OUTCOME
dtrain <- xgb.DMatrix(data = X_train, label = labels)
dtest <- xgb.DMatrix(data = X_test, label=ts_label)
xgbcv <- xgb.cv(data = dtrain, nrounds = 100,
nfold = 5, showsd = T, stratified = T,
print_every_n = 10, early_stopping_rounds = 20,
maximize = F)
xgb1 = xgb.train(data = dtrain,
max.depth = 2,
eta = 1,
nthread = 2,
nrounds = 10,
objective = "binary:logistic",
verbose = 1)
xgbpred1 <- predict(xgb1,dtest)
xgbpred_res1 <- as.numeric(xgbpred1 > 0.5,1,0)
MC_xgb = confusionMatrix(as.factor(xgbpred_res1), as.factor(ts_label))
MC_xgb
mat <- xgb.importance (feature_names = colnames(X_train), model = xgb1)
xgb.plot.importance (importance_matrix = mat)
MC_logreg$table
MC_logreg$dots
MC_logreg$byClass
MC_logreg$table
MC_tree$table
MC_rf$table
MC_xgb$table
View(MC_logreg)
View(MC_logreg$table)
set.seed(42) # set random seed for reproducibility
# Train/test split
trainIndex = sample(1:nrow(data_processed), size = round(0.8*nrow(data_processed)), replace=FALSE)
train = data_processed[trainIndex ,]
test = data_processed[-trainIndex ,]
logreg <- glm(OUTCOME~., data=train, family=binomial)
summary(logreg)
proba_logreg <- predict(logreg, newdata=test, type="response")
pred_logreg <- ifelse(proba_logreg<0.5, 0, 1)
pred_logreg <- factor(pred_logreg)
MC_logreg <- confusionMatrix(as.factor(test$OUTCOME), pred_logreg)
MC_logreg
decision_tree <- rpart(OUTCOME~., data=train)
rpart.plot(decision_tree)
pred_tree <- predict(object=decision_tree, newdata=test, type="class")
MC_tree <- confusionMatrix(test$OUTCOME, pred_tree)
MC_tree
rf <- randomForest(as.factor(OUTCOME)~., data=train, proximity=TRUE)
pred_rf <- predict(rf, test)
MC_rf <- confusionMatrix(test$OUTCOME, pred_rf)
MC_rf
train$OUTCOME = as.numeric(levels(train$OUTCOME))[train$OUTCOME]
test$OUTCOME = as.numeric(levels(test$OUTCOME))[test$OUTCOME]
X_train <- model.matrix(~ INCOME + VEHICLE_YEAR +
SPEEDING_VIOLATIONS  + AGE + DRIVING_EXPERIENCE +
CREDIT_SCORE + MARRIED + ANNUAL_MILEAGE + DUIS +
VEHICLE_OWNERSHIP + CHILDREN +
PAST_ACCIDENTS,data=train)
X_test <- model.matrix(~ INCOME + VEHICLE_YEAR +
SPEEDING_VIOLATIONS  + AGE + DRIVING_EXPERIENCE +
CREDIT_SCORE + MARRIED + ANNUAL_MILEAGE + DUIS +
VEHICLE_OWNERSHIP + CHILDREN +
PAST_ACCIDENTS,data=test)
labels <- train$OUTCOME
ts_label <- test$OUTCOME
dtrain <- xgb.DMatrix(data = X_train, label = labels)
dtest <- xgb.DMatrix(data = X_test, label=ts_label)
xgbcv <- xgb.cv(data = dtrain, nrounds = 100,
nfold = 5, showsd = T, stratified = T,
print_every_n = 10, early_stopping_rounds = 20,
maximize = F)
xgb1 = xgb.train(data = dtrain,
max.depth = 2,
eta = 1,
nthread = 2,
nrounds = 10,
objective = "binary:logistic",
verbose = 1)
xgbpred1 <- predict(xgb1,dtest)
xgbpred_res1 <- as.numeric(xgbpred1 > 0.5,1,0)
MC_xgb = confusionMatrix(as.factor(xgbpred_res1), as.factor(ts_label))
MC_xgb
mat <- xgb.importance (feature_names = colnames(X_train), model = xgb1)
xgb.plot.importance (importance_matrix = mat)
MC_xgb
View(head(processed_data))
View(head(data_processed))
# Compute the list of correlation values
newdata <- one_hot(as.data.table(data))
mat <- cor(newdata)
correlations <- as.data.frame(mat[,37])
names(correlations) <- c("Outcome")
correlations <- correlations %>% arrange(desc(abs(Outcome)))
# Display all variables which correlation with OUTCOME is more than 20%
select <- correlations %>%
filter(abs(Outcome)>0.2 & abs(Outcome) != 1)
select
View(select)
library(forecast)
library(lmtest)
library(fpp)
library(tseries)
data <- read.table("data_gendron.csv", header=T, sep=';')
head(data)
summary(data)
tempe <- ts(data$Temperature)
str(tempe)
Acf(tempe)
Pacf(tempe)
tsdisplay(tempe)
tsdisplay(diff(tempe, 12))
ts <- diff(tempe, 12)
adf.test(ts)
kpss.test(ts)
train <- data$Temperature[1:1091]
train <- ts(train)
test <- data$Temperature[1092:1364]
test <- ts(test)
st_train <- diff(train, 12)
st_test <- diff(test, 12)
tsdisplay(train, lag.max = 50)
tsdisplay(test, lag.max = 50)
tsdisplay(st_train, lag.max = 30)
tsdisplay(st_test, lag.max = 30)
model <- Arima(train,order=c(1,0,1), seasonal=c(2,1,0))
summary(model)
plot(model$residuals, ylab="Residuals")
abline(0,0)
tsdiag(model)
hist(residuals(model), freq = F, col = "grey")
curve(dnorm(x, mean = mean(residuals(model)),sd = sd(residuals(model))), col = 2, add = TRUE)
qqnorm(residuals(model))
qqline(residuals(model),col=2)
shapiro.test(residuals(model))
